{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0ae4ed5",
   "metadata": {},
   "source": [
    "# 1. Installation of the Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f29231",
   "metadata": {},
   "source": [
    "## 1.1 Creating a Virtual Environment\n",
    "\n",
    "To isolate the project dependencies and ensure that they are properly installed and configured, it is a best practice to create a virtual environment.\n",
    "\n",
    "### What is a Virtual Environment?\n",
    "\n",
    "A virtual environment is a self-contained directory that contains a specific version of Python and any packages or modules needed for a particular project. By using a virtual environment, you can isolate the dependencies of a project from the global Python environment on your machine, which makes it easier to manage and avoid version conflicts.\n",
    "\n",
    "### Creating a Virtual Environment with Conda\n",
    "\n",
    "You can create a virtual environment using `conda` \n",
    "\n",
    "To create a new virtual environment called `tensorflow_env` using `conda`, run the following cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0271dc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda create --name tensorflow_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e5d1c1",
   "metadata": {},
   "source": [
    "### Activating and Deactivating a Virtual Environment\n",
    "\n",
    "To activate the virtual environment, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e36f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda activate tensorflow_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31ddaf6",
   "metadata": {},
   "source": [
    "To deactivate the virtual environment, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e13bc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!deactivate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdad918",
   "metadata": {},
   "source": [
    "## 1.2 Installing Required Packages\n",
    "\n",
    "Once you have activated the virtual environment, install the required packages running the following cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac35da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install dependencies in the new environment:\n",
    "!conda install -c jmcmurray os\n",
    "!conda install -c anaconda pathlib\n",
    "!conda install -c conda-forge matplotlib\n",
    "!conda install -c anaconda numpy\n",
    "!conda install -c anaconda seaborn\n",
    "!conda install -c conda-forge tensorflow\n",
    "!conda install -c anaconda ipython\n",
    "!conda install -c anaconda urllib3\n",
    "!conda install -c anaconda zipfile36\n",
    "!conda install -c anaconda scipy\n",
    "!conda install -c conda-forge librosa\n",
    "!conda install -c conda-forge pydub\n",
    "!conda install -c anaconda cudatoolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe0440d",
   "metadata": {},
   "source": [
    "## 1.3 Importing the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72f78ad3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae1136ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pydub'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8815/2452654170.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# import librosa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpydub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAudioSegment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pydub'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import librosa\n",
    "import subprocess\n",
    "from pydub import AudioSegment\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from IPython import display\n",
    "\n",
    "# Set the seed value for experiment reproducibility.\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed2dbb4",
   "metadata": {},
   "source": [
    "#  2. Importing the MDVR-KCL Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935a0096",
   "metadata": {},
   "source": [
    "## 2.1 Project Overview\n",
    "In this project, we will be working with the **Mobile Device Voice Recordings at King's College London (MDVR-KCL) dataset**, which includes voice recordings from both early and advanced Parkinson's disease patients as well as from healthy controls.\n",
    "\n",
    "Our goal is to use machine learning to classify the recordings and differentiate between healthy controls and Parkinson's disease patients, as well as between early and advanced stages of the disease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e544b24",
   "metadata": {},
   "source": [
    "## 2.2 Dataset\n",
    "\n",
    "To work with this dataset, we will be downloading a zipfile from the Zenodo data repository that contains the voice recordings. We will then extract the files and use them to train and test our machine learning models.\n",
    "\n",
    "### Download\n",
    "To **download** the dataset into your computer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61cddef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!TEST\n",
    "\n",
    "# specify the URL to download the zip file\n",
    "url = \"https://zenodo.org/record/2867216/files/26_29_09_2017_KCL.zip?download=1\"\n",
    "\n",
    "# Define the subdirectory to extract the contents of the zip file\n",
    "subdir = \"data\"\n",
    "\n",
    "# create a subdirectory named \"data\" \n",
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\")\n",
    "    print(f\"Created subdir data\")\n",
    "    \n",
    "# Download the zip file from the URL\n",
    "filename, headers = urllib.request.urlretrieve(url)\n",
    "print(f\"Downloaded file\")\n",
    "\n",
    "# Extract the contents of the zip file to the subdirectory\n",
    "with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "    zip_ref.extractall(subdir)\n",
    "print(f\"Extracted files to data\")\n",
    "\n",
    "# Remove the downloaded zip file\n",
    "os.remove(filename)\n",
    "\n",
    "print(f\"Dataset downloaded and extracted to subdir data. Zip file removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e11716",
   "metadata": {},
   "source": [
    "### Lable the dataset\n",
    "\n",
    "Lables used to distinguish between: \n",
    "- Healthy Control Subjects **(HC)**\n",
    "- Subjects with Parkinson's disease **(PD)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe1a5ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['HC', 'PD']\n"
     ]
    }
   ],
   "source": [
    "labels = ['HC','PD']\n",
    "print('Labels:', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "483358ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that returns 0 if hc and 1 if pd\n",
    "def get_diagnosis(filename):\n",
    "    if 'hc' in filename:\n",
    "        return 0\n",
    "    elif 'pd' in filename:\n",
    "        return 1\n",
    "    else:\n",
    "        raise ValueError('Invalid filename format: ' + filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348ce913",
   "metadata": {},
   "source": [
    "### Convert the audio to 16bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b8de9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio files converted\n"
     ]
    }
   ],
   "source": [
    "# Navigate to the data directory\n",
    "data_dir = 'data/26-29_09_2017_KCL/'\n",
    "\n",
    "# Save the current working directory\n",
    "original_dir = os.getcwd()\n",
    "\n",
    "#change dir to get the files\n",
    "os.chdir(data_dir)\n",
    "\n",
    "# Convert all WAV files to 16-bit\n",
    "for diagnosis in ['HC', 'PD']:\n",
    "    for mode in ['ReadText', 'SpontaneousDialogue']:\n",
    "        input_dir = os.path.join(mode, diagnosis)\n",
    "        output_dir = os.path.join('16-bit', mode, diagnosis)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        for file in os.listdir(input_dir):\n",
    "            if file.endswith('.wav'):\n",
    "                input_file = os.path.join(input_dir, file)\n",
    "#                 print('INPUT FILE: ',input_file)\n",
    "\n",
    "                output_file = os.path.join(output_dir, f\"{os.path.splitext(file)[0]}.16.wav\")\n",
    "#                 print('OUTPUT FILE: ', output_file)\n",
    "\n",
    "                subprocess.call(['ffmpeg', '-i', input_file, '-ac', '1', '-ar', '16000', '-sample_fmt', 's16','-loglevel','quiet' ,output_file]) \n",
    "# Change back to the original directory\n",
    "os.chdir(original_dir)\n",
    "print('Audio files converted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a578db1",
   "metadata": {},
   "source": [
    "### Extract audio files and shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d49e816a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total examples: 73\n",
      "Number of examples in HC: 42\n",
      "Number of examples in PD: 31\n",
      "Example file tensor: tf.Tensor(b'data/26-29_09_2017_KCL/16-bit/ReadText/HC/ID21_hc_0_0_0.16.wav', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "#Extract audio files\n",
    "data_dir = pathlib.Path(\"data/26-29_09_2017_KCL/16-bit\")\n",
    "filenames = tf.io.gfile.glob(str(data_dir / '*' / '*' / '*'))\n",
    "\n",
    "# create a list of label indices for each file\n",
    "label_indices = [labels.index(pathlib.Path(filename).parent.name) for filename in filenames]\n",
    "# print(label_indices)\n",
    "\n",
    "#Shuffle audio files to avoid bias\n",
    "filenames_random = tf.random.shuffle(filenames)\n",
    "num_samples = len(filenames_random)\n",
    "\n",
    "# Create a tf.data.Dataset from the shuffled filenames\n",
    "dataset = tf.data.Dataset.from_tensor_slices(filenames_random)\n",
    "\n",
    "\n",
    "print('Number of total examples:', num_samples)\n",
    "print('Number of examples in HC:',\n",
    "      sum([len(tf.io.gfile.listdir(str(data_dir / subdir / labels[0]))) for subdir in ['ReadText', 'SpontaneousDialogue']]))\n",
    "print('Number of examples in PD:',\n",
    "      sum([len(tf.io.gfile.listdir(str(data_dir / subdir / labels[1]))) for subdir in ['ReadText', 'SpontaneousDialogue']]))\n",
    "\n",
    "print('Example file tensor:', filenames_random[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86f77ab",
   "metadata": {},
   "source": [
    "## 3. Data Augmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31829b1",
   "metadata": {},
   "source": [
    "Because we have a dataset of only 73 audio files, we are going to use various data augmentation techniques:\n",
    "- Adding Gaussian background noise\n",
    "- Changing the pitch of the recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7f78459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_pitch(audio, sample_rate, pitch_factor):\n",
    "    # Convert the audio tensor to a numpy array\n",
    "    audio = np.array(audio)\n",
    "\n",
    "    # Convert the numpy array to an AudioSegment\n",
    "    audio_segment = AudioSegment(\n",
    "        data=audio.tobytes(),\n",
    "        sample_width=audio.dtype.itemsize,\n",
    "        frame_rate=sample_rate,\n",
    "        channels=1\n",
    "    )\n",
    "\n",
    "    # Adjust the pitch using the pydub library\n",
    "    adjusted_audio_segment = audio_segment._spawn(\n",
    "        audio_segment.raw_data,\n",
    "        overrides={\n",
    "            \"frame_rate\": int(sample_rate * pitch_factor)\n",
    "        }\n",
    "    )\n",
    "    adjusted_audio_segment = adjusted_audio_segment.set_frame_rate(sample_rate)\n",
    "\n",
    "    # Convert the adjusted AudioSegment back to a numpy array\n",
    "    adjusted_audio = np.frombuffer(adjusted_audio_segment.raw_data, np.int16)\n",
    "\n",
    "    # Convert the numpy array back to a tensor\n",
    "    adjusted_audio = tf.convert_to_tensor(adjusted_audio)\n",
    "\n",
    "    # Normalize the audio tensor\n",
    "    adjusted_audio = tf.cast(adjusted_audio, dtype=tf.float32)\n",
    "    adjusted_audio = tf.divide(adjusted_audio, 32768.0)  # scale the audio to the range [-1, 1]\n",
    "\n",
    "    return adjusted_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8f43b929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"args_0:0\", shape=(), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-20 22:00:45.150160: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at decode_wav_op.cc:52 : INVALID_ARGUMENT: Can only read 16-bit WAV files, but received 24\n",
      "2023-02-20 22:00:45.150586: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at decode_wav_op.cc:52 : INVALID_ARGUMENT: Can only read 16-bit WAV files, but received 24\n",
      "2023-02-20 22:00:45.155789: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at decode_wav_op.cc:52 : INVALID_ARGUMENT: Can only read 16-bit WAV files, but received 24\n",
      "2023-02-20 22:00:45.161476: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at decode_wav_op.cc:52 : INVALID_ARGUMENT: Can only read 16-bit WAV files, but received 24\n",
      "2023-02-20 22:00:45.164372: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at decode_wav_op.cc:52 : INVALID_ARGUMENT: Can only read 16-bit WAV files, but received 24\n",
      "2023-02-20 22:00:45.169494: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at decode_wav_op.cc:52 : INVALID_ARGUMENT: Can only read 16-bit WAV files, but received 24\n",
      "2023-02-20 22:00:45.169581: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at decode_wav_op.cc:52 : INVALID_ARGUMENT: Can only read 16-bit WAV files, but received 24\n",
      "2023-02-20 22:00:45.170113: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at decode_wav_op.cc:52 : INVALID_ARGUMENT: Can only read 16-bit WAV files, but received 24\n",
      "2023-02-20 22:00:45.171105: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at decode_wav_op.cc:52 : INVALID_ARGUMENT: Can only read 16-bit WAV files, but received 24\n",
      "2023-02-20 22:00:45.174760: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at decode_wav_op.cc:52 : INVALID_ARGUMENT: Can only read 16-bit WAV files, but received 24\n",
      "2023-02-20 22:00:45.176754: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at decode_wav_op.cc:52 : INVALID_ARGUMENT: Can only read 16-bit WAV files, but received 24\n",
      "2023-02-20 22:00:45.180215: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at decode_wav_op.cc:52 : INVALID_ARGUMENT: Can only read 16-bit WAV files, but received 24\n",
      "2023-02-20 22:00:45.180981: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at decode_wav_op.cc:52 : INVALID_ARGUMENT: Can only read 16-bit WAV files, but received 24\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Can only read 16-bit WAV files, but received 24\n\t [[{{node DecodeWav}}]] [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(load_audio_file)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Iterate over the dataset and print the file path for each element\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m audio, file_path \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio:\u001b[39m\u001b[38;5;124m'\u001b[39m, audio, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_path:\u001b[39m\u001b[38;5;124m'\u001b[39m, file_path)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:766\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    765\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    767\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[1;32m    768\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:749\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[0;32m--> 749\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    754\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    755\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[1;32m    756\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3017\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3015\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3016\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 3017\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3018\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[1;32m   3019\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7209\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7208\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7209\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Can only read 16-bit WAV files, but received 24\n\t [[{{node DecodeWav}}]] [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define a function to load the audio files and apply data augmentation\n",
    "def load_audio_file(file_path, pitch_factor=0.0, noise_factor=0.0):\n",
    "   \n",
    "    # Load the audio file using tf.audio.decode_wav\n",
    "    audio, sample_rate = tf.audio.decode_wav(tf.io.read_file(file_path))\n",
    "\n",
    "     # Add background noise to the audio file\n",
    "    if noise_factor > 0.0:\n",
    "        noise = tf.random.normal(tf.shape(audio), stddev=noise_factor)\n",
    "        audio = tf.cast(audio, dtype=tf.float32)\n",
    "        audio = audio + noise\n",
    "\n",
    "\n",
    "    # Change the pitch of the audio file\n",
    "    if pitch_factor != 0.0:\n",
    "        audio = change_pitch(audio, sample_rate, pitch_factor)\n",
    "\n",
    "    # Return the audio data and label (if available)\n",
    "    return audio,file_path\n",
    "\n",
    "pitch_factor = 0.2\n",
    "noise_factor = 0.1\n",
    "# Use map to apply the load_audio_file function to the dataset\n",
    "dataset = dataset.map(load_audio_file)\n",
    "\n",
    "# Iterate over the dataset and print the file path for each element\n",
    "for audio, file_path in dataset:\n",
    "    print('audio:', audio, 'file_path:', file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8743cc",
   "metadata": {},
   "source": [
    "### Divide files into training, validation and tests sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1b48aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size 58\n",
      "Validation set size 7\n",
      "Test set size 7\n"
     ]
    }
   ],
   "source": [
    "#! EL DATASET ES MUY PEQUE COMO PARA HACER ESTO. SOLUCIÓN APARTE\n",
    "#Ratio of 80:10:10 respectively\n",
    "train_files = filenames[:58]\n",
    "val_files = filenames[58: 58 + 7]\n",
    "test_files = filenames[-7:]\n",
    "\n",
    "print('Training set size', len(train_files))\n",
    "print('Validation set size', len(val_files))\n",
    "print('Test set size', len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c88188",
   "metadata": {},
   "source": [
    "Leer los archivos de audio y sus etiquetas.\n",
    "En esta sección, preprocesará el conjunto de datos, creando tensores decodificados para las formas de onda y las etiquetas correspondientes. Tenga en cuenta que:\n",
    "\n",
    "Cada archivo WAV contiene datos de series temporales con un número determinado de muestras por segundo.\n",
    "Cada muestra representa la amplitud de la señal de audio en ese momento específico.\n",
    "En un sistema de 16 bits , como los archivos WAV en el conjunto de datos mini Speech Commands, los valores de amplitud oscilan entre -32 768 y 32 767.\n",
    "La frecuencia de muestreo para este conjunto de datos es de 16 kHz.\n",
    "La forma del tensor devuelto por tf.audio.decode_wav es [samples, channels] , donde channels son 1 para mono o 2 para estéreo. El conjunto de datos de mini comandos de voz solo contiene grabaciones mono.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06c753c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_path = 'data/26-29_09_2017_KCL/ReadText/PD/ID02_pd_2_0_0.wav'\n",
    "test_file = tf.io.read_file(test_file_path)\n",
    "\n",
    "test_audio, _ = tf.audio.decode_wav(contents=test_file)\n",
    "test_audio.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
